{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from seq2seq.rnn_seq2seq import create_seq2seq_model, create_seq2seq_experiment_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! rm -r ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell_num = 2\n",
    "\n",
    "cell = \"LSTMCell\"\n",
    "num_layers = 2\n",
    "num_units = 16\n",
    "bidirectional = True\n",
    "attention = True\n",
    "residual_connections = True\n",
    "residual_dense = True\n",
    "\n",
    "vocab_size = 10\n",
    "emb_size = 5\n",
    "\n",
    "training_mode = \"scheduled_sampling_embedding\"\n",
    "scheduled_sampling_probability = 0.5\n",
    "\n",
    "inference_mode = \"beam\"\n",
    "beam_width = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_sequences(length_from, length_to,\n",
    "                     vocab_lower, vocab_upper,\n",
    "                     batch_size):\n",
    "    \"\"\" Generates batches of random integer sequences,\n",
    "        sequence length in [length_from, length_to],\n",
    "        vocabulary in [vocab_lower, vocab_upper]\n",
    "    \"\"\"\n",
    "    if length_from > length_to:\n",
    "        raise ValueError('length_from > length_to')\n",
    "\n",
    "    def random_length():\n",
    "        if length_from == length_to:\n",
    "            return length_from\n",
    "        return np.random.randint(length_from, length_to + 1)\n",
    "    \n",
    "    while True:\n",
    "        yield [\n",
    "            np.random.randint(low=vocab_lower,\n",
    "                              high=vocab_upper,\n",
    "                              size=random_length()).tolist()\n",
    "            for _ in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.contrib.learn.python.learn.learn_io.generator_io import generator_input_fn\n",
    "from seq2seq.input.generator_io import generator_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator_py():\n",
    "    def generator():\n",
    "        data_gen = random_sequences(9, 9, 2, 9, 1)\n",
    "        for _ in range(1000):\n",
    "            data = next(data_gen)\n",
    "            \n",
    "            data = np.array(data[0], dtype=np.int32)\n",
    "            data_len = len(data)\n",
    "            data_len = np.array(data_len, dtype=np.int32)\n",
    "\n",
    "            yield {\n",
    "                \"inputs\": data,\n",
    "                \"inputs_length\": data_len,\n",
    "                \"targets\": data,\n",
    "                \"targets_length\": data_len\n",
    "            }\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dg_train_input_fn = generator_input_fn(\n",
    "    x=data_generator_py(), \n",
    "    target_key=[\"targets\", \"targets_length\"], \n",
    "    batch_size=16, shuffle=False, num_epochs=None, \n",
    "    queue_capacity=128, num_threads=2, \n",
    "    pad_data=True)\n",
    "dg_test_input_fn = generator_input_fn(\n",
    "    x=data_generator_py(), \n",
    "    target_key=[\"targets\", \"targets_length\"], \n",
    "    batch_size=16, shuffle=False, num_epochs=None, \n",
    "    queue_capacity=128, num_threads=1, \n",
    "    pad_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_fn = create_seq2seq_experiment_fn(\n",
    "    dg_train_input_fn, dg_test_input_fn, \n",
    "    train_steps=int(2e4), eval_steps=int(1e2), min_eval_frequency=int(1e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_option = 0.5  # Yeap, Estimator can use memory limitations\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_option)\n",
    "\n",
    "run_config = tf.contrib.learn.RunConfig(\n",
    "    session_config=tf.ConfigProto(gpu_options=gpu_options),\n",
    "    model_dir=\"./logs\")\n",
    "\n",
    "hparams = tf.contrib.training.HParams(\n",
    "    cell_num=cell_num,\n",
    "    vocab_size=vocab_size, embedding_size=emb_size,\n",
    "    cell=cell, num_layers=num_layers, num_units=num_units,\n",
    "    bidirectional=bidirectional, attention=attention,\n",
    "    residual_connections=residual_connections, \n",
    "    residual_dense=residual_dense,\n",
    "    training_mode=training_mode,\n",
    "    learning_rate=1e-4,\n",
    "    lr_decay_steps=100000,\n",
    "    lr_decay_koef=0.99,\n",
    "    gradient_clip=10.0,\n",
    "    scheduled_sampling_probability=scheduled_sampling_probability,\n",
    "    inference_mode=inference_mode,\n",
    "    beam_width=beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scitator/.conda/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'global_step': 20001, 'loss': 1.1041856}, [])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR) # cause we don't want to log anything\n",
    "tf.contrib.learn.learn_runner.run(\n",
    "    experiment_fn=experiment_fn,\n",
    "    run_config=run_config,\n",
    "    schedule=\"train_and_evaluate\",\n",
    "    hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg_pred_input_fn = generator_input_fn(\n",
    "    x=data_generator_py(), \n",
    "    target_key=[\"targets\", \"targets_length\"], \n",
    "    batch_size=1, shuffle=False, num_epochs=None, \n",
    "    queue_capacity=128, num_threads=1, \n",
    "    pad_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = create_seq2seq_model(config=run_config, hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(dg_pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred = next(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': array([[4, 4, 4],\n",
       "        [4, 4, 4],\n",
       "        [4, 4, 4],\n",
       "        [8, 8, 8],\n",
       "        [4, 4, 4],\n",
       "        [3, 3, 3],\n",
       "        [4, 4, 4],\n",
       "        [4, 3, 4],\n",
       "        [4, 4, 3],\n",
       "        [1, 1, 1]], dtype=int32),\n",
       " 'score': array([[-0.23443063, -2.6063211 , -3.04360557],\n",
       "        [-0.56695133, -2.49730635, -2.89267826],\n",
       "        [-1.11435843, -2.39209199, -2.48673224],\n",
       "        [-2.16065478, -2.32451248, -2.53673363],\n",
       "        [-2.94408631, -3.16454864, -3.27134466],\n",
       "        [-3.68032479, -3.80859733, -4.13525438],\n",
       "        [-3.97157574, -4.36352539, -4.97107124],\n",
       "        [-4.51236534, -5.02003574, -5.15970612],\n",
       "        [-5.23004818, -5.45817852, -5.61370182],\n",
       "        [-5.23004818, -5.45817852, -5.6137023 ]], dtype=float32)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
